# -*- coding: utf-8 -*-
"""script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/popescumadalin0/novel-anime-generator/blob/main/script.ipynb

# Anime Face Generation

##CNGAN

Configuration:
"""
from AnimeData import AnimeData
from data_loader import download_data
from utils import make_array, gallery

#hyperparameters
LATENT_DIM = 100
IMAGE_SIZE = 64
BATCH_SIZE = 128
NUM_EPOCHS = 1
LEARNING_RATE = 0.0002
BETA1 = 0.5
BETA2 = 0.999
NUM_WORKERS = 0

import os

import matplotlib.pyplot as plt
import pandas as pd
import torch
from torch import optim
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm.notebook import tqdm

download_data()

root = './faces/images/'

from PIL import Image
import glob
image_list = []
rows = []
for filename in glob.glob('./images/*.jpg'):
    im=Image.open(filename)
    rows.append([filename])
    image_list.append(filename)

print(len(image_list))

display_no_images = 10

array = make_array()
result = gallery(array, display_no_images)
plt.figure(figsize=(8,8))
plt.imshow(result)
plt.show()

df = pd.DataFrame(rows)
df.to_csv('data.csv', index=False, header = None)

stats = (BETA1, BETA1, BETA1), (BETA1, BETA1, BETA1)
transform = transforms.Compose([
                                transforms.CenterCrop(IMAGE_SIZE),
                                transforms.Resize(IMAGE_SIZE, interpolation=2),
                                transforms.ToTensor(),
                                transforms.Normalize(*stats)])

trainset = AnimeData(root='./data.csv', transform=transform)
trainloader = DataLoader(trainset, BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)

if torch.cuda.is_available():
    device=torch.device('cuda')
else:
    device=torch.device('cpu')

#dcgan

dCGANDiscriminator=DCGANDiscriminator(3).to(device)

"""## Generator

"""



dCGANGenerator=DCGANGenerator(LATENT_DIM).to(device)
# random latent tensors
noise = torch.randn(BATCH_SIZE, LATENT_DIM, 1, 1).to(device)

fake_images = dCGANGenerator(noise)
print(fake_images.shape)

show_images(fake_images)

for real_images in tqdm(trainloader):
    real_images=(real_images).to(device)

show_images(real_images)


loss_fn = torch.nn.MSELoss()

# Create optimizers for the discriminator D and generator G
opt_d = optim.Adam(dCGANDiscriminator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))
opt_g = optim.Adam(dCGANGenerator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))


sample_dir = 'generated'
os.makedirs(sample_dir, exist_ok=True)

fixed_latent = torch.randn(IMAGE_SIZE, LATENT_DIM, 1, 1, device=device)

"""## Training GAN to generate anime faces
Training will involve alternating between training the discriminator and the generator. Functions `real_loss` and `fake_loss` to help you calculate the discriminator losses.

"""

# Complete the training function
losses_g = []
losses_d = []
real_scores = []
fake_scores = []

#Train the GAN
train(dCGANDiscriminator,dCGANGenerator,opt_d,opt_g,epochs=NUM_EPOCHS)

##Visualize loss curve of D and G
fig, ax = plt.subplots()
plt.plot(losses_g, label='Discriminator', alpha=BETA1)
plt.plot(losses_d, label='Generator', alpha=BETA1)
plt.title("Training Losses")
plt.legend()

"""## StyleGAN

### Hyperparameters
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset
from torchvision.utils import save_image
import os
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm

LATENT_DIM = 512
NUM_EPOCHS = 1
BATCH_SIZE = 32
BETA1 = 0.0
BETA2 = 0.99
LEARNING_RATE = 0.0002
IMAGE_SIZE = 64
NUM_WORKERS = 0

"""Mapping network: Maps latent code z to intermediate latent code w

This is a key innovation in StyleGAN that enables better disentanglement
"""

class MappingNetwork(nn.Module):

    def __init__(self, latent_dim=512, hidden_dim=512, num_layers=8):
        super(MappingNetwork, self).__init__()

        layers = []
        for i in range(num_layers):
            layers.extend([
                nn.Linear(latent_dim if i == 0 else hidden_dim, hidden_dim),
                nn.LeakyReLU(0.2)
            ])

        self.mapping = nn.Sequential(*layers)

    def forward(self, z):
        return self.mapping(z)

"""Adaptive Instance Normalization (AdaIN)
    Transfers style information from w to the feature maps
"""

class AdaptiveInstanceNorm(nn.Module):

    def __init__(self, num_features, w_dim):
        super(AdaptiveInstanceNorm, self).__init__()
        self.norm = nn.InstanceNorm2d(num_features, affine=False)

        # Style modulation parameters
        self.style_scale = nn.Linear(w_dim, num_features)
        self.style_bias = nn.Linear(w_dim, num_features)

    def forward(self, x, w):
        # Normalize the input
        x = self.norm(x)

        # Apply style modulation
        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)
        style_bias = self.style_bias(w).unsqueeze(2).unsqueeze(3)

        return style_scale * x + style_bias

"""Injects learned noise into feature maps for stochastic variation"""

class NoiseInjection(nn.Module):

    def __init__(self):
        super(NoiseInjection, self).__init__()
        self.weight = nn.Parameter(torch.zeros(1))

    def forward(self, x, noise=None):
        if noise is None:
            batch, _, height, width = x.shape
            noise = torch.randn(batch, 1, height, width, device=x.device)

        return x + self.weight * noise

"""StyleGAN synthesis block with AdaIN and noise injection"""

class StyleBlock(nn.Module):


    def __init__(self, in_channels, out_channels, w_dim, upsample=True):
        super(StyleBlock, self).__init__()

        self.upsample = upsample
        if upsample:
            self.upsample_layer = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)

        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.noise = NoiseInjection()
        self.adain = AdaptiveInstanceNorm(out_channels, w_dim)
        self.activation = nn.LeakyReLU(0.2)

    def forward(self, x, w, noise=None):
        if self.upsample:
            x = self.upsample_layer(x)

        x = self.conv(x)
        x = self.noise(x, noise)
        x = self.adain(x, w)
        x = self.activation(x)

        return x

"""Generator"""

class StyleGANGenerator(nn.Module):
    """
    StyleGAN Generator with mapping network and synthesis network
    Generates 64x64 anime faces
    """

    def __init__(self, latent_dim=512, w_dim=512, img_channels=3):
        super(StyleGANGenerator, self).__init__()

        self.latent_dim = latent_dim
        self.w_dim = w_dim

        # Mapping network
        self.mapping = MappingNetwork(latent_dim, w_dim)

        # Initial constant input (learned)
        self.constant_input = nn.Parameter(torch.randn(1, 512, 4, 4))

        # Synthesis network
        self.style_blocks = nn.ModuleList([
            StyleBlock(512, 512, w_dim, upsample=False),  # 4x4
            StyleBlock(512, 512, w_dim, upsample=True),  # 8x8
            StyleBlock(512, 256, w_dim, upsample=True),  # 16x16
            StyleBlock(256, 128, w_dim, upsample=True),  # 32x32
            StyleBlock(128, 64, w_dim, upsample=True),  # 64x64
        ])

        # RGB output layers for each resolution
        self.to_rgb = nn.ModuleList([
            nn.Conv2d(512, img_channels, 1),
            nn.Conv2d(512, img_channels, 1),
            nn.Conv2d(256, img_channels, 1),
            nn.Conv2d(128, img_channels, 1),
            nn.Conv2d(64, img_channels, 1),
        ])

    def forward(self, z, return_w=False):
        batch_size = z.shape[0]

        # Map to intermediate latent space
        w = self.mapping(z)

        # Start with constant input
        x = self.constant_input.repeat(batch_size, 1, 1, 1)

        # Progressive synthesis
        for i, style_block in enumerate(self.style_blocks):
            x = style_block(x, w)

        # Convert to RGB
        img = self.to_rgb[-1](x)
        img = torch.tanh(img)

        if return_w:
            return img, w
        return img

    def style_mixing(self, z1, z2, mix_layer=2):
        """
        Style mixing: use different w vectors at different layers
        """
        batch_size = z1.shape[0]

        w1 = self.mapping(z1)
        w2 = self.mapping(z2)

        x = self.constant_input.repeat(batch_size, 1, 1, 1)

        for i, style_block in enumerate(self.style_blocks):
            w = w1 if i < mix_layer else w2
            x = style_block(x, w)

        img = self.to_rgb[-1](x)
        img = torch.tanh(img)

        return img

"""Discriminator"""

class StyleGANDiscriminator(nn.Module):
    """
    Progressive discriminator for StyleGAN
    """

    def __init__(self, img_channels=3):
        super(StyleGANDiscriminator, self).__init__()

        self.model = nn.Sequential(
            # 64x64 -> 32x32
            nn.Conv2d(img_channels, 64, 4, 2, 1),
            nn.LeakyReLU(0.2),

            # 32x32 -> 16x16
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),

            # 16x16 -> 8x8
            nn.Conv2d(128, 256, 4, 2, 1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2),

            # 8x8 -> 4x4
            nn.Conv2d(256, 512, 4, 2, 1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2),

            # 4x4 -> 1x1
            nn.Conv2d(512, 1, 4, 1, 0),
        )

    def forward(self, img):
        return self.model(img).view(-1)

"""## Training StyleGAN"""

class AnimeFaceDataset(Dataset):
    """Dataset class for loading anime face images"""

    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.images = [f for f in os.listdir(root_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = os.path.join(self.root_dir, self.images[idx])
        image = Image.open(img_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image


def train_stylegan(num_epochs=1,
                   batch_size=32, latent_dim=512, lr=LEARNING_RATE):

    # Initialize models
    styleGANGenerator = StyleGANGenerator(latent_dim=latent_dim).to(device)
    styleGANDiscriminator = StyleGANDiscriminator().to(device)

    start_idx=1

    # Initialize weights
    def weights_init(m):
        if isinstance(m, (nn.Conv2d, nn.Linear)):
            nn.init.normal_(m.weight.data, 0.0, 0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias.data, 0)

    styleGANGenerator.apply(weights_init)
    styleGANDiscriminator.apply(weights_init)

    # Optimizers
    optimizer_G = optim.Adam(styleGANGenerator.parameters(), lr=lr, betas=(BETA1, BETA2))
    optimizer_D = optim.Adam(styleGANDiscriminator.parameters(), lr=lr, betas=(BETA1, BETA2))

    # Loss function
    criterion = nn.BCEWithLogitsLoss()

    # Fixed noise for visualization
    fixed_noise = torch.randn(64, latent_dim, device=device)

    # Training history
    g_losses = []
    d_losses = []

    print("Starting StyleGAN training...")

    for epoch in range(num_epochs):
        epoch_g_loss = 0
        epoch_d_loss = 0

        pbar = tqdm(trainloader, desc=f'Epoch {epoch + 1}/{num_epochs}')
        for i, real_imgs in enumerate(pbar):
            batch_size_curr = real_imgs.size(0)
            real_imgs = real_imgs.to(device)

            # Labels
            real_labels = torch.ones(batch_size_curr, device=device)
            fake_labels = torch.zeros(batch_size_curr, device=device)

            # ---------------------
            #  Train Discriminator
            # ---------------------
            optimizer_D.zero_grad()

            # Real images
            real_output = styleGANDiscriminator(real_imgs)
            d_loss_real = criterion(real_output, real_labels)

            # Fake images
            z = torch.randn(batch_size_curr, latent_dim, device=device)
            fake_imgs = styleGANGenerator(z)
            fake_output = styleGANDiscriminator(fake_imgs.detach())
            d_loss_fake = criterion(fake_output, fake_labels)

            # Total discriminator loss
            d_loss = d_loss_real + d_loss_fake
            d_loss.backward()
            optimizer_D.step()

            # -----------------
            #  Train Generator
            # -----------------
            optimizer_G.zero_grad()

            # Generate fake images
            z = torch.randn(batch_size_curr, latent_dim, device=device)
            fake_imgs = styleGANGenerator(z)
            fake_output = styleGANDiscriminator(fake_imgs)

            # Generator loss
            g_loss = criterion(fake_output, real_labels)
            g_loss.backward()
            optimizer_G.step()

            # Update progress bar
            epoch_g_loss += g_loss.item()
            epoch_d_loss += d_loss.item()
            pbar.set_postfix({'G_loss': g_loss.item(), 'D_loss': d_loss.item()})



        # Calculate average losses
        avg_g_loss = epoch_g_loss / len(trainloader)
        avg_d_loss = epoch_d_loss / len(trainloader)
        g_losses.append(avg_g_loss)
        d_losses.append(avg_d_loss)

        print(f"Epoch [{epoch + 1}/{num_epochs}] - G_loss: {avg_g_loss:.4f}, D_loss: {avg_d_loss:.4f}")

        # Save generated images
        save_samples(epoch+start_idx, fixed_latent,styleGANGenerator,'styleGANGenerator', show=True)

        state_dis = {'styleGANDiscriminator_model': styleGANDiscriminator.state_dict(), 'epoch': epoch}
        state_gen = {'styleGANGenerator_model': styleGANGenerator.state_dict(), 'epoch': epoch}
        if not os.path.isdir('checkpoint'):
            os.mkdir('checkpoint')
        torch.save(state_dis, 'checkpoint/'+'styleGANDiscriminator'+str(epoch+1)) #each epoch
        torch.save(state_gen, 'checkpoint/'+'styleGANGenerator__'+str(epoch+1)) #each epoch

    # Plot training losses
    plt.figure(figsize=(10, 5))
    plt.plot(g_losses, label='Generator Loss')
    plt.plot(d_losses, label='Discriminator Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('StyleGAN Training Losses')
    plt.close()

    print("Training complete!")
    return styleGANGenerator, styleGANDiscriminator


styleGANGenerator, styleGANDiscriminator = train_stylegan(
        num_epochs=NUM_EPOCHS,
        batch_size=BATCH_SIZE,
        latent_dim=LATENT_DIM
    )

"""Latent space interpolation"""

def generate_samples(generator, num_samples=16, latent_dim=512):
    """Generate samples from trained StyleGAN"""
    device = next(generator.parameters()).device
    generator.eval()

    with torch.no_grad():
        z = torch.randn(num_samples, latent_dim, device=device)
        fake_imgs = generator(z)
        print(fake_imgs)


def latent_space_interpolation(generator, num_steps=10, latent_dim=512):
    """
    Create smooth interpolation between two random points in latent space
    """
    device = next(generator.parameters()).device
    generator.eval()

    with torch.no_grad():
        # Generate two random latent codes
        z1 = torch.randn(1, latent_dim, device=device)
        z2 = torch.randn(1, latent_dim, device=device)

        # Interpolate
        alphas = torch.linspace(0, 1, num_steps, device=device)
        interpolated_images = []

        for alpha in alphas:
            z = (1 - alpha) * z1 + alpha * z2
            img = generator(z)
            interpolated_images.append(img)

        # Save grid
        grid = torch.cat(interpolated_images, dim=0)
        print(grid)


print("\nGenerating samples...")
generate_samples(styleGANGenerator, num_samples=16, latent_dim=LATENT_DIM)

# Latent space interpolation
print("\nCreating latent space interpolation...")
latent_space_interpolation(styleGANGenerator, num_steps=10, latent_dim=LATENT_DIM)

"""Comparison between both 2 models"""

def compare_with_dcgan(stylegan_path, dcgan_path, output_dir='comparison'):
    """
    Generate samples from both models for visual comparison
    """
    os.makedirs(output_dir, exist_ok=True)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Load StyleGAN
    stylegan = StyleGANGenerator().to(device)
    stylegan.load_state_dict(torch.load(stylegan_path, map_location=device)['generator_state_dict'])
    stylegan.eval()

    dcgan = DCGANGenerator().to(device)
    dcgan.load_state_dict(torch.load(dcgan_path, map_location=device)['generator_state_dict'])
    dcgan.eval()

    with torch.no_grad():
        # Generate samples
        z = torch.randn(16, 512, device=device)

        # StyleGAN samples
        stylegan_samples = stylegan(z)
        save_image(stylegan_samples, f'{output_dir}/stylegan_samples.png', nrow=4, normalize=True)

        # DCGAN samples (reshape z if needed)
        z_dcgan = z.unsqueeze(2).unsqueeze(3) if len(dcgan(z[:1]).shape) == 4 else z
        dcgan_samples = dcgan(z_dcgan)
        save_image(dcgan_samples, f'{output_dir}/dcgan_samples.png', nrow=4, normalize=True)

    print(f"Comparison samples saved to {output_dir}")